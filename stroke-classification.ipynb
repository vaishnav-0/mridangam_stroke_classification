{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(stroke-classification)=\n",
    "# Stroke classification\n",
    "The task of percussion stroke classification has been, historically, the principal target on the timbre-side of computational analysis of Indian Art Music. As seen in the [instrumentation presentation](instrumentation), the musical arrangement in Indian Art Music is quite well-defined, while there is an important scarcity of good-quality data (specially monphonic recordings) for many of the Carnatic and Hindustani specific instruments. These factors combined with the importance of the different stroke types in the main percussion instruments, have given research importance on mridangam {cite}`anantapadmanabhan_mridangam_2013, mridangam_stroke` and tabla stroke classification {cite}`4way_tabla`.\n",
    "\n",
    "We will go through examples of these tasks in this walkthrough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import extras and supress warnings to keep the tutorial clean\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "## Importing compiam to the project\n",
    "import compiam\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mridangam stroke classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[   INFO   ] MusicExtractorSVM: no classifier models were configured by default\n"
     ]
    }
   ],
   "source": [
    "from compiam.timbre.stroke_classification import MridangamStrokeClassification\n",
    "\n",
    "msc = MridangamStrokeClassification()  # Let's use msc for simplicity\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by loading the mridangam stroke dataset. Since ``MridangamStrokeClassification``is based on the Mridangam Stroke Dataset, `compiam` includes a specific function to load the dataset and integrate it to the pipeline."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6976/6976 [00:00<00:00, 10035.84it/s]\n"
     ]
    }
   ],
   "source": [
    "msc.load_mridangam_dataset(data_home=\"../audio/mir_dataset/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{note}\n",
    "This function does not return a dataloader. Instead, the dataloader lives within the tool class. We will see how this works in the following steps of this walkthrough. You may check out the [MridangamStrokeClassification documentation](https://mtg.github.io/compIAM/source/timbre.html#compiam.timbre.stroke_classification.mridangam_stroke_classification.MridangamStrokeClassification) to learn how we do take advantage of the dataloader in this tool class.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['bheem', 'cha', 'dheem', 'dhin', 'num', 'ta', 'tha', 'tham', 'thi', 'thom']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print list of available mirdangam strokes in the dataset\n",
    "msc.list_strokes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's train and evaluate a very basic model to perform classification of mridangam strokes. We first use a util function in the `mirdata` Dataset class to separate the Mridangam Stroke Dataset in paticular splits. We will use [``get_random_track_splits``](https://mirdata.readthedocs.io/en/latest/source/mirdata.html#mirdata.core.Dataset.get_random_track_splits), since this dataset does not have pre-determined splits, and we will create these randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('225104',\n Track(\n   audio_path=\"../audio/mir_dataset/mridangam_stroke_1.5/B/225104__akshaylaya__thi-b-323.wav\",\n   stroke_name=\"thi\",\n   tonic=\"B\",\n   track_id=\"225104\",\n   audio: The track's audio\n \n         Returns,\n ))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading tracks for the mirdangam dataset\n",
    "mridangam_tracks = msc.dataset.load_tracks()\n",
    "\n",
    "# Getting list of id per split\n",
    "# NOTE: We use (0.9, 0.1): two splits including 90% and 10% of the whole dataset\n",
    "split_dict = msc.dataset.get_random_track_splits(\n",
    "    splits=(0.9, 0.1),\n",
    "    split_names=(\"train\", \"validation\")\n",
    ")\n",
    "\n",
    "# Get track dictionaries given the created splits\n",
    "train_split = {x: mridangam_tracks[x] for x in split_dict[\"train\"]}\n",
    "evaluation_split = {y: mridangam_tracks[y] for y in split_dict[\"validation\"]}\n",
    "\n",
    "# Let's get random track from the created evaluation split\n",
    "random.choice(list(evaluation_split.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our class will assume that the entire dataset is used for the training process. We need to update the dataset in the class with the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msc.mridangam_tracks = train_split\n",
    "msc.mridangam_ids = list(train_split.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Let's now train the model!** We will train Support Vector Machine (SVM) model using `scikit learn`. The mridangam stroke classification tool in `compiam` uses the [MusicExtraction in Essentia](https://essentia.upf.edu/streaming_extractor_music.html) to compute low-level features from the stroke recordings and feed the model.\n",
    "\n",
    "```{note}\n",
    "You can also train a different model and compare the performance. We offer other options (see [the documentation of the tool](https://mtg.github.io/compIAM/source/timbre.html#mridangam-stroke-classification)), but feel free to open a Pull Request in `compiam` to add more models to the available options.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:11,  1.40s/it][ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      " 50%|█████     | 5/10 [00:11<00:13,  2.78s/it][ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      " 60%|██████    | 6/10 [00:17<00:14,  3.74s/it][ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      " 70%|███████   | 7/10 [00:24<00:14,  4.78s/it][ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      " 90%|█████████ | 9/10 [00:40<00:06,  6.98s/it][ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "[ WARNING  ] clipping oddtoevenharmonicenergyratio to maximum allowed value\n",
      "100%|██████████| 10/10 [00:44<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model successfully trained with accuracy 91% in the testing set\n"
     ]
    }
   ],
   "source": [
    "svm_accuracy = msc.train_model(model_type=\"svm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**The model has been trained!** We have also got the testing accuracy returned in case we want to store it, re-train the model again using different settings, and compare. \n",
    "\n",
    "Now we can predict the stroke on a particular list of instances. First, we need to get the list of paths for the `mirdata` dataset split we generated a few steps earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get paths from created evaluation split\n",
    "eval_paths = [evaluation_split[x].audio_path for x in list(evaluation_split.keys())]\n",
    "\n",
    "# Compute prediction from list of paths\n",
    "# prediction = msc.predict(eval_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mfcc0.mean  mfcc0.dev  mfcc1.mean  mfcc1.dev  mfcc2.mean  mfcc2.dev  \\\n",
      "0 -737.903198  61.411499   151.83432   7.143669  -34.474564    4.08334   \n",
      "\n",
      "   mfcc3.mean  mfcc3.dev  mfcc4.mean  mfcc4.dev  ...  \\\n",
      "0    9.920733   2.599565    0.318367    0.43185  ...   \n",
      "\n",
      "   lowLevel.spectral_strongpeak.mean  lowLevel.spectral_strongpeak.dev  \\\n",
      "0                           0.088152                          0.061141   \n",
      "\n",
      "   lowLevel.zerocrossingrate.mean  lowLevel.zerocrossingrate.dev  \\\n",
      "0                        0.017578                            0.0   \n",
      "\n",
      "   tonal.tuning_frequency.mean  tonal.tuning_frequency.dev  \\\n",
      "0                   439.491974                         0.0   \n",
      "\n",
      "   lowLevel.barkbands.mean  lowLevel.barkbands.dev  lowLevel.scvalleys.mean  \\\n",
      "0                 0.000207                0.000568                -7.096732   \n",
      "\n",
      "   lowLevel.scvalleys.dev  \n",
      "0                1.428743  \n",
      "\n",
      "[1 rows x 90 columns]\n",
      "{'../audio/mir_dataset/mridangam_stroke_1.5/D#/229769__akshaylaya__thi-dsh-206.wav': 'thi'}\n"
     ]
    }
   ],
   "source": [
    "prediction_1 = msc.predict(eval_paths[30])\n",
    "print(prediction_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../audio/mir_dataset/mridangam_stroke_1.5/D#/228925__akshaylaya__dhin-dsh-126.wav',\n",
      " 'dhin')\n"
     ]
    }
   ],
   "source": [
    "# Visualise and evaluate some predictions from the model output\n",
    "pprint(random.choice(list(prediction.items())))\n",
    "# pprint(random.choice(list(prediction.items())))\n",
    "# pprint(random.choice(list(prediction.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the file paths of this validation files we can already see the actual stroke that is present in the recording, so we can evaluate how good our model classified the mridangam strokes. Otherwise, we can also get the actual tonic using the `mirdata` loader and a particular track ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Track(\n  audio_path=\"../../../audio/mir_dataset/mridangam_stroke_1.5/C/226097__akshaylaya__thi-c-026.wav\",\n  stroke_name=\"thi\",\n  tonic=\"C\",\n  track_id=\"226097\",\n  audio: The track's audio\n\n        Returns,\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc.dataset.choice_track()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We note that the ID has been directly taken from the file name of the stroke recordings. Let's use that to compare, for a random prediction, the predicted and ground-truth stroke annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_14657/941500523.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Selecting a random example from the predicted files\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mpredicted_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredicted_stroke\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"total:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "# Selecting a random example from the predicted files\n",
    "predicted_file, predicted_stroke = random.choice(list(prediction.items()))\n",
    "\n",
    "print(\"total:\", len(prediction));\n",
    "\n",
    "true_count = 0\n",
    "for k in prediction:\n",
    "    if prediction[k] == evaluation_split[os.path.basename(k).split(\"__\")[0]].stroke_name:\n",
    "        true_count += 1\n",
    "\n",
    "print(\"true:\", true_count)\n",
    "\n",
    "# Getting the ID from filepath\n",
    "identifier = os.path.basename(predicted_file).split(\"__\")[0]\n",
    "\n",
    "# Comparing target and estimation\n",
    "# if evaluation_split[identifier].stroke_name == predicted_stroke:\n",
    "#     print(\"Nice! Predicted stroke in {}\\n coincides with ground-truth {}\"\\\n",
    "#         .format(\n",
    "#             os.path.basename(predicted_file),\n",
    "#             evaluation_split[identifier].stroke_name\n",
    "#         )\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"Missed! Predicted stroke in {}\\n does NOT coincide with ground-truth {}\"\\\n",
    "#         .format(\n",
    "#             os.path.basename(predicted_file),\n",
    "#             evaluation_split[identifier].stroke_name\n",
    "#         )\n",
    "#     )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/bheem1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha_thi_thom_num_2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/num2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha_thi_thom_num_3.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/chappu3.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/thi4.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/bheem1-2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha_thi_thom_num_2_2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/num1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/ki3.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/ki1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/bheem2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/dheem1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/bheem3.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/dheem3.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/chappu1_cutoff_down.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/dhin1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/thom2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/thi3.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/chappu1_cutoff.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/thom1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/chappu2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/dheem2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/ta2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/thom3.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/thi1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/chappu1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha_thi_thom_num2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/thi2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/chappu2_cutoff.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha_thi_thom_num1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/dhin2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/ta1.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/chappu4.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/ki2.wav', '/home/vaichu/projects/IITM/Mridangam_Sir/june6_210/tha_thi_thom_num_4.wav']\n"
     ]
    }
   ],
   "source": [
    "def absoluteFilePaths(directory):\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            yield os.path.abspath(os.path.join(dirpath, f))\n",
    "\n",
    "\n",
    "samples = list(absoluteFilePaths(\"../../../Mridangam_Sir/june6_210\"))\n",
    "\n",
    "print(samples)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from compiam.timbre.stroke_classification.mridangam_stroke_classification.stroke_features import (\n",
    "    features_for_pred,\n",
    ")\n",
    "import importlib\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tha2.wav\": \"thi\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(compiam.timbre.stroke_classification.mridangam_stroke_classification.stroke_features)\n",
    "pre = msc.predict(samples)\n",
    "print(json.dumps({x.split(\"/\")[-1]: v for (x, v) in pre.items()}, indent=4, sort_keys=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}